{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be90136-1477-448b-b3ec-c7b96c760fed",
   "metadata": {},
   "source": [
    "**1. Importing necessary libraries for data manipulation, visualization, and modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a97f4e-c913-4625-a480-94b45223ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling data in DataFrame format\n",
    "import pandas as pd\n",
    "\n",
    "# For numerical computations\n",
    "import numpy as np                \n",
    "\n",
    "# For creating visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For advanced visualizations\n",
    "import seaborn as sns\n",
    "\n",
    "# For splitting data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For K-Nearest Neighbors regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# For data normalization or standardization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# For hyperparameter tuning using grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# For Gradient Boosting regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# For performing cross-validation\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25099d-4af9-41da-9d80-ae152592b2ac",
   "metadata": {},
   "source": [
    "**2. Loading Data and Ensuring Data Quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d35e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# df = pd.read_csv(r\"C:\\Users\\38095\\Documents\\GitHub\\Project_6\\winequality-red.csv\")\n",
    "df = pd.read_csv(\"data/winequality-red.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22dce7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffe696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ea22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset (number of rows and columns)\n",
    "df.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0134b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types of each column in the dataset\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71029848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c0ec7-94b8-48d4-80af-4aff7a036eb3",
   "metadata": {},
   "source": [
    "**3. Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (independent variables) and target (dependent variable)\n",
    "\n",
    "features = df.drop(columns=[\"quality\"])\n",
    "target = df[\"quality\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN regressor with 10 neighbors\n",
    "knn = KNeighborsRegressor(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared score on the test set\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ef21b-1685-4e7d-9eca-75b6569ca855",
   "metadata": {},
   "source": [
    "R-squared (coefficient of determination) measures the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features) in the model.\n",
    "In this case, an R-squared of 0.1221 indicates that approximately 12.21% of the variance in the wine quality can be explained by the features included in the model.\n",
    "This value is relatively low, suggesting that the model may not be capturing a significant portion of the variance in the target variable, and there may be room for improvement in the model's predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8b58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature scaling using MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "normalizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841fc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing sets\n",
    "X_train_norm = normalizer.transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled arrays back to dataframes\n",
    "X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n",
    "X_test_norm = pd.DataFrame(X_test_norm, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN regressor with 10 neighbors\n",
    "knn = KNeighborsRegressor(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daace65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the scaled data\n",
    "knn.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared score on the scaled test set\n",
    "knn.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310bfcf-7e5a-4110-92c4-5b50bede1bbb",
   "metadata": {},
   "source": [
    "The score obtained from the KNN model represents the coefficient of determination (R-squared) on the test set, which measures the proportion of the variance in the target variable (wine quality) that is explained by the features in the model.\n",
    "In this case, the score of 0.1984 indicates that approximately 19.84% of the variance in wine quality can be explained by the features included in the KNN model.\n",
    "A higher R-squared value closer to 1 would indicate a better fit of the model to the data, suggesting that the features are more effective in predicting wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac897e",
   "metadata": {},
   "source": [
    "**4. Model Development and Initial Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap to identify highly correlated features\n",
    "corr=np.abs(df.corr())\n",
    "\n",
    "#Set up mask for triangle representation\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,  vmax=1,square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot = corr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03130b",
   "metadata": {},
   "source": [
    "Fixed acidity and pH (corr. = 0.68): Both features are related to the acidity of the wine. Including both may lead to redundancy of information. It may be sufficient to keep only one of them.\n",
    "\n",
    "Free sulfur dioxide and total sulfur dioxide (corr. = 0.67): Both features pertain to the sulfur dioxide content in wine, where the total content includes the free form. Keeping only one of them could suffice.\n",
    "\n",
    "Density and residual sugar: High sugar content may affect the density of wine. If information about sugar content (residual sugar) is available, density may not be as crucial a feature.\n",
    "\n",
    "Volatile acidity and sulphates (corr. = 0.26): High volatile acidity levels may lead to increased sulphur dioxide levels. Considering this, one of these features could be excluded.\n",
    "\n",
    "To build a wine quality prediction model, it's crucial to select informative features relevant to the target variable.\n",
    "Features like:\n",
    "- citric acid\n",
    "- density\n",
    "- total sulfur dioxide\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- alcohol\n",
    "can contribute significantly to predicting wine quality.\n",
    "\n",
    "These attributes impact the taste, freshness, acidity, and preservation of wine, making them relevant for modeling. Additionally, some of these features are interrelated or share similarities. For example, both free and total sulfur dioxide levels are relevant for preserving wine, while pH and acidity are closely related. \n",
    "\n",
    "fixed acidity\n",
    "most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n",
    "\n",
    "volatile acidity\n",
    "the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n",
    "\n",
    "citric acid\n",
    "found in small quantities, citric acid can add 'freshness' and flavor to wines\n",
    "\n",
    "residual sugar\n",
    "the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet\n",
    "\n",
    "chlorides\n",
    "the amount of salt in the wine\n",
    "\n",
    "free sulfur dioxide\n",
    "the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n",
    "\n",
    "total sulfur dioxide\n",
    "amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n",
    "\n",
    "density\n",
    "the density of water is close to that of water depending on the percent alcohol and sugar content\n",
    "\n",
    "pH\n",
    "describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n",
    "\n",
    "sulphates\n",
    "a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on correlation analysis\n",
    "# Drop highly correlated features to reduce redundancy\n",
    "X_train_reduced = X_train_norm.drop(columns = [\"residual sugar\", \"chlorides\", \"free sulfur dioxide\",\"pH\", \"sulphates\"])\n",
    "X_test_reduced = X_test_norm.drop(columns = [\"residual sugar\", \"chlorides\", \"free sulfur dioxide\",\"pH\", \"sulphates\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN regressor with 10 neighbors\n",
    "knn = KNeighborsRegressor(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ec9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on reduced features\n",
    "knn.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51286116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the reduced feature test set\n",
    "pred_new = knn.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared score on the reduced feature test set\n",
    "knn.score(X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511071f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN regressor with 30 neighbors\n",
    "knn = KNeighborsRegressor(n_neighbors=30)\n",
    "knn.fit(X_train_reduced, y_train)\n",
    "knn.score(X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9108f",
   "metadata": {},
   "source": [
    "**5. Advanced Modeling**\n",
    "\n",
    "Experiment with more powerful models, such as Ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced modeling using ensemble techniques\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor,AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b479d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging regressor\n",
    "bagging_reg = BaggingRegressor(DecisionTreeRegressor(max_depth=30),\n",
    "                               n_estimators=100,\n",
    "                               max_samples = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_reg.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d99826",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bagging_reg.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "print(\"MAE\", mean_absolute_error(pred, y_test))\n",
    "print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n",
    "print(\"R2 score\", bagging_reg.score(X_test_reduced, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb71be-4645-468d-b9d2-ea7fdff77640",
   "metadata": {},
   "source": [
    "The Mean Absolute Error (MAE) is approximately 0.472, indicating the average absolute difference between the predicted and true wine quality ratings.\n",
    "The Root Mean Squared Error (RMSE) is approximately 0.638, representing the square root of the average squared differences between the predicted and true wine quality ratings.\n",
    "The R-squared score is approximately 0.311, which indicates that around 31.1% of the variance in the wine quality ratings can be explained by the features included in the model.\n",
    "These evaluation metrics provide insights into the performance of the model in predicting wine quality, with the R-squared score suggesting a moderate level of predictive capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree regressor\n",
    "tree = DecisionTreeRegressor(max_depth  = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca5afa-023f-47a3-82e9-0ac593f460be",
   "metadata": {},
   "source": [
    "The decision tree regressor with a maximum depth of 5 achieves an R-squared score of approximately 0.1564.\n",
    "This score indicates that around 15.64% of the variance in the wine quality ratings can be explained by the features included in the model.\n",
    "Despite having a limited depth, the decision tree shows some predictive capability, albeit modest, in capturing the relationship between the features and the wine quality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a703b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regressor\n",
    "forest = RandomForestRegressor(n_estimators=100,\n",
    "                             max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403aafa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e027b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(X_test_reduced)\n",
    "forest.score(X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907f4e9-5081-4a9e-a69f-bba6ca7faff5",
   "metadata": {},
   "source": [
    "The random forest regressor achieves an R-squared score of approximately 0.3077 on the test set.\n",
    "Compared to the decision tree regressor with a maximum depth of 5, which had an R-squared score of approximately 0.1564, the random forest regressor demonstrates better predictive performance, explaining around 30.77% of the variance in the wine quality ratings based on the features included in the model.\n",
    "This improvement in predictive capability suggests that the ensemble of decision trees in the random forest model is better able to capture the complex relationships between the features and the wine quality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=20),\n",
    "                            n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_reg.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = ada_reg.predict(X_test_reduced)\n",
    "\n",
    "print(\"MAE\", mean_absolute_error(pred, y_test))\n",
    "print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n",
    "print(\"R2 score\", ada_reg.score(X_test_reduced, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2412560-c2d0-4da7-b437-597de7eeaf55",
   "metadata": {},
   "source": [
    "The AdaBoostRegressor achieves an R-squared score of approximately 0.1501 on the test set.\n",
    "Compared to the Random Forest Regressor with an R-squared score of approximately 0.3077 and the Decision Tree Regressor with an R-squared score of approximately 0.1564, the AdaBoostRegressor demonstrates weaker predictive performance.\n",
    "With an R-squared score of 0.1501, the AdaBoostRegressor explains approximately 15.01% of the variance in the wine quality ratings based on the features included in the model.\n",
    "This indicates that the AdaBoost ensemble method, in this case, may not be as effective as the Random Forest or Decision Tree models in capturing the underlying relationships between the features and the wine quality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ce92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor\n",
    "gb_reg = GradientBoostingRegressor(max_depth=20,\n",
    "                                   n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_reg.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gb_reg.predict(X_test_reduced)\n",
    "\n",
    "print(\"MAE\", mean_absolute_error(pred, y_test))\n",
    "print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n",
    "print(\"R2 score\", gb_reg.score(X_test_reduced, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b618b-5bdb-45da-ab8c-d4da97ae2c0e",
   "metadata": {},
   "source": [
    "The GradientBoostingRegressor achieves an R-squared score of approximately -0.238 on the test set.\n",
    "Compared to previous models:\n",
    "- AdaBoostRegressor with an R-squared score of approximately 0.1501,\n",
    "- Random Forest Regressor with an R-squared score of approximately 0.3077,\n",
    "- Decision Tree Regressor with an R-squared score of approximately 0.1564, the GradientBoostingRegressor demonstrates the weakest predictive performance, with a negative R-squared score.\n",
    "An R-squared score below zero indicates that the model performs worse than a model that simply predicts the mean of the target variable.\n",
    "This suggests that the GradientBoostingRegressor may not be suitable for capturing the relationships between the features and the wine quality ratings in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52dc75f",
   "metadata": {},
   "source": [
    "**5. Hyperparameter Tuning and Model Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'n_estimators': [50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new GradientBoostingRegressor with the best parameters\n",
    "best_gb_reg = GradientBoostingRegressor(max_depth=5, n_estimators=150)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_gb_reg.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = best_gb_reg.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation metrics\n",
    "print(\"MAE\", mean_absolute_error(pred, y_test))\n",
    "print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n",
    "print(\"R2 score\", r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00fce2-5de4-4875-b5d8-7a3193f08735",
   "metadata": {},
   "source": [
    "The GradientBoostingRegressor model with optimized hyperparameters achieves:\n",
    "- Mean Absolute Error (MAE) of approximately 0.526,\n",
    "- Root Mean Squared Error (RMSE) of approximately 0.850,\n",
    "- R-squared score of approximately 0.215 on the test set.\n",
    "Compared to the previous GradientBoostingRegressor model with default hyperparameters, the model with optimized hyperparameters shows a slight improvement in predictive performance, with a higher R-squared score, indicating better fit to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GradientBoostingRegressor model with optimized hyperparameters\n",
    "best_gb_reg = GradientBoostingRegressor(max_depth=5, n_estimators=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation with 5 folds\n",
    "scores = cross_val_score(best_gb_reg, X_train_reduced, y_train, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean R2 score across all folds\n",
    "print(\"Mean R2 Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc2a8a-3194-4cf3-ad73-719b04741329",
   "metadata": {},
   "source": [
    "The mean R2 score, approximately 0.3525, derived from cross-validation, signifies the overall predictive capability of the \n",
    "GradientBoostingRegressor model. Compared to prior individual R2 scores—AdaBoostRegressor around 0.1501,\n",
    "GradientBoostingRegressor demonstrates comparatively stronger predictive performance.\n",
    "This indicates that the model captures a significant portion of the target variable's variance, suggesting its potential effectiveness, although further refinements may be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0beb1e",
   "metadata": {},
   "source": [
    "**6. The outcome**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b25278-0a96-4366-9db7-a6edcdf17288",
   "metadata": {},
   "source": [
    "Let's compare the performance metrics of the machine learning models provided in the code and draw a general conclusion:\n",
    "\n",
    "K-Nearest Neighbors (KNN):\n",
    "\n",
    "R-squared: 0.1984 (after scaling)\n",
    "MAE and RMSE not provided.\n",
    "\n",
    "BaggingRegressor (with Decision Trees):\n",
    "\n",
    "R-squared: 0.311\n",
    "MAE: 0.472\n",
    "RMSE: 0.638\n",
    "\n",
    "DecisionTreeRegressor:\n",
    "\n",
    "R-squared: 0.1564\n",
    "MAE and RMSE not provided.\n",
    "\n",
    "RandomForestRegressor:\n",
    "\n",
    "R-squared: 0.3077\n",
    "MAE and RMSE not provided.\n",
    "\n",
    "AdaBoostRegressor (with Decision Trees):\n",
    "\n",
    "R-squared: 0.1501\n",
    "MAE and RMSE not provided.\n",
    "\n",
    "GradientBoostingRegressor:\n",
    "\n",
    "R-squared: -0.238\n",
    "MAE: not provided\n",
    "RMSE: not provided\n",
    "\n",
    "GradientBoostingRegressor with Hyperparameter Optimization:\n",
    "\n",
    "R-squared: 0.215\n",
    "MAE: 0.526\n",
    "RMSE: 0.850\n",
    "\n",
    "Cross-validation score (with Optimized GradientBoostingRegressor):\n",
    "\n",
    "Mean R-squared: 0.3525\n",
    "General Conclusion:\n",
    "\n",
    "The BaggingRegressor and RandomForestRegressor models show comparable and relatively good quality with an R-squared around 0.31, indicating that about 31% of the variance in the target variable is explained by the model.\n",
    "The GradientBoostingRegressor with optimized hyperparameters also demonstrates acceptable quality with an R-squared around 0.215 and a mean R-squared during cross-validation of about 0.3525.\n",
    "Other models like KNN, DecisionTreeRegressor, and AdaBoostRegressor exhibit lower prediction quality.\n",
    "GradientBoostingRegressor without hyperparameter optimization shows poor quality with a negative R-squared, indicating that the model performs worse than simply predicting the mean.\n",
    "\n",
    "***Thus, the most effective models for predicting wine quality on this dataset are BaggingRegressor, RandomForestRegressor, and GradientBoostingRegressor with optimized hyperparameters.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054f85f-9023-4830-ad28-7252f8500f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
